{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# BERT plus BERT text classiffication safeguarding 3 categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "```\n",
        "This notebooks trains a model to identify 3 categories on written reviews:\n",
        "```\n",
        "- No safeguarding content (label=0)\n",
        "- Safeguarding content low risk (label=1)\n",
        "- Safeguarding content high risk (label=2)\n",
        "\n",
        "### Data\n",
        "The data used for training uses a mix of published NHS UK reviews as it is assumed that none include safeguarding content, NHS UK reviews and modified open source posts from social media that were marked by specialist moderators as high risk or low risk. It also includes augmented data from the previous and not real comments written to simulate the level of risk needed (high risk). See more details on the model card corresponding to this model\n",
        "\n",
        "### Encoding\n",
        "A BERT tokenizer was used to encode the data and includes the id, the token and the attention mask\n",
        "\n",
        "### Model\n",
        "A neural network architecture designed and trained for text classification using BERT was used to trained the model. \n",
        "\n",
        "\n",
        "> - Features for the model will come solely from the text of the review as a vector representation. Downloading a pre-train BERT model, use it to transform written sentences into numerical vectors. These vectors capture the essence of the sentences in a form that the model can understand. \n",
        "> \n",
        "\n",
        "> - Fine-tuning the Model. Use a BERT model that has been pre-trained for classification tasks. Then, finetune it for our task by training it further on our own set of labeled texts. These texts are divided into two types: those that relate to safeguarding issues and those that do not. This step helps the model learn from our specific examples and get better at distinguishing between the two types.\n",
        "> \n",
        "\n",
        "> - Register the model.\n",
        "> \n",
        "\n",
        "> - The deployment of the model will be done on a separate script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import types\n",
        "Import all the Azure Machine Learning types that I'll need "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1707487205354
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.51.0\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "\n",
        "# Third-party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Azure ML imports\n",
        "import azureml.core\n",
        "from azureml.core import Experiment, Model, Dataset, Workspace\n",
        "\n",
        "# Configuring Seaborn\n",
        "sns.set()  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1707487209957
        }
      },
      "outputs": [],
      "source": [
        "#Create a workspace object from the existing Azure Machine Learning workspace\n",
        "workspace = Workspace.from_config()\n",
        "print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the pipeline infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1707487214971
        }
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "from tqdm import trange\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1707487217623
        }
      },
      "outputs": [],
      "source": [
        "# create experiment and start logging to a new run in the experiment\n",
        "from azureml.core import Experiment\n",
        "\n",
        "name = 'safeguarding_bert_bert3' \n",
        "\n",
        "short_hand_name = \"safeguarding3\"\n",
        "\n",
        "experiment = Experiment(workspace = workspace, name = name)\n",
        "\n",
        "# Start logging data from the experiment\n",
        "run = experiment.start_logging(snapshot_directory=None)\n",
        "run.display_name = name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1707487226137
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "safeguarding = Dataset.get_by_name(workspace, name='safeguarding_for_training_v1')\n",
        "safeguarding = safeguarding.to_pandas_dataframe()\n",
        "safeguarding = safeguarding.sample(100)\n",
        "\n",
        "# We extract text and label values:\n",
        "text = safeguarding.text.values\n",
        "labels = safeguarding.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encode data\n",
        "### Download tokenizer from pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1707487238244
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case = True\n",
        "    )\n",
        "# This is a deep neural network with 12 layers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1707487247743
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  ''' #this docstring is great\n",
        "  Tokenizes input text into format compatible with BERT models.\n",
        "  \n",
        "  Parameters: - input_text (str): Text to tokenize. - tokenizer\n",
        "  (transformers.BertTokenizer): Tokenizer instance to use.\n",
        "\n",
        "  Returns: transformers.tokenization_utils_base.BatchEncoding object containing:\n",
        "  - input_ids (torch.Tensor): Tensor of token ids to be fed to a model. -\n",
        "  attention_mask (torch.Tensor): Tensor of indices specifying which tokens\n",
        "  should be attended to by the model. - token_type_ids (torch.Tensor, optional):\n",
        "  Tensor of segment ids to differentiate multiple sequences. Not used for single\n",
        "  sequence tasks.\n",
        "  \n",
        "  The function ensures that the tokenized output is truncated or padded to a max\n",
        "  length of 512 tokens, includes special tokens (like [CLS], [SEP]), and returns\n",
        "  PyTorch tensors.\n",
        "  '''\n",
        "  return tokenizer.encode_plus( \n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt' # Return pytorch tensors.\n",
        "                   )\n",
        "                 \n",
        "for sample in text:\n",
        "    encoding_dict = preprocessing(sample, tokenizer)\n",
        "    token_id.append(encoding_dict['input_ids']) \n",
        "    attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels) #The BERT PyTorch interface requires that the data be in torch tensors rather than Python lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1707487256918
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 8 #Memory issues on the environment didn't allow for larger batch sizes\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets \n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader  \n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selected hyperparameters based on the recommendations from the BERT paper (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin) [https://arxiv.org/pdf/1810.04805.pdf]:\n",
        "The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:\n",
        "\n",
        "- Batch size: 16, 32\n",
        "\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "\n",
        "- Number of epochs: 2, 3, 4\n",
        "\n",
        "Define some functions to assess validation metrics (accuracy, precision, recall and specificity) during the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def b_tp(preds, labels):\n",
        "    \"\"\"\n",
        "    Calculates the count of correct predictions for True Positives (TP) and True Negatives (TN).\n",
        "\n",
        "    Args:\n",
        "        preds (list or array): Predicted labels.\n",
        "        labels (list or array): Actual ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of correct predictions where predicted labels match the actual labels.\n",
        "    \"\"\"\n",
        "    return sum(preds == labels for preds, labels in zip(preds, labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def b_tn(preds, labels):\n",
        "    \"\"\"\n",
        "    Returns True Negatives (TN): count of correct predictions of actual class 0.\n",
        "\n",
        "    Args:\n",
        "        preds (list or array): Predicted labels.\n",
        "        labels (list or array): Actual ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of correct predictions where the predicted and actual labels are both 0.\n",
        "    \"\"\"\n",
        "    return sum(1 for pred, label in zip(preds, labels) if pred == label == 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def b_metrics(preds, labels):\n",
        "    \"\"\"\n",
        "    Calculates and returns the accuracy of predictions.\n",
        "\n",
        "    Args:\n",
        "        preds (numpy array): Predicted probability or logits for each class from the model, where the last axis indexes the class.\n",
        "        labels (numpy array): Actual ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the predictions calculated as (TP + TN) / N, where N is the total number of samples.\n",
        "    \"\"\"\n",
        "    preds = np.argmax(preds, axis=1).flatten()  # Convert probabilities or logits to class predictions\n",
        "    labels = labels.flatten()                   # Flatten labels if not already 1D\n",
        "    tp = b_tp(preds, labels)                    # Calculate True Positives \n",
        "    tn = b_tn(preds, labels)                    # Calculate True Negatives\n",
        "    b_accuracy = (tp + tn) / len(labels)        # Compute accuracy\n",
        "    return b_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download transformers.BertForSequenceClassification, which is a BERT model with a linear layer for sentence classification (or regression) on top of the pooled output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1707484150407
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81da0459c708466a8d9f0196f2566efa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(bert_model.parameters(), \n",
        "                              lr = 3e-5, \n",
        "                              eps = 1e-08 \n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "bert_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1707484171213
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 3\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    bert_model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = bert_model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    bert_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = bert_model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "          \n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1707484195700
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps)) \n",
        "print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy))) \n",
        "\n",
        "run.log('Accuracy', float(sum(val_accuracy)/len(val_accuracy)) ) \n",
        "run.log('Train_Loss', float(tr_loss / nb_tr_steps)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save the freshly trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1669130696547
        }
      },
      "outputs": [],
      "source": [
        "#save model for later inference\n",
        "torch.save(bert_model.state_dict(), './safeguardingxb.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# See it working (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1707486913206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: No safeguarding, Probability: 0.9988974332809448\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Corrected device assignment\n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=3,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "\n",
        "bert_model.load_state_dict(torch.load('./safeguardinglvexp8b.pkl', map_location=device))  # Load model to the specified device\n",
        "bert_model = bert_model.to(device)  # Ensure model is on the correct device\n",
        "bert_model.eval()  # Set model to evaluation mode\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case=True\n",
        ")\n",
        "\n",
        "def preprocessing(input_text, tokenizer): \n",
        "    return tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',  # Updated for newer transformers versions\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "def safeguarding_prediction(text_to_classify):\n",
        "    encoding = preprocessing(text_to_classify, tokenizer)\n",
        "\n",
        "    test_ids = encoding['input_ids'].to(device)  # Move to the correct device\n",
        "    test_attention_mask = encoding['attention_mask'].to(device)  # Move to the correct device\n",
        "\n",
        "    with torch.no_grad():  # Forward pass\n",
        "        output = bert_model(test_ids, token_type_ids=None, attention_mask=test_attention_mask)\n",
        "\n",
        "    logits = output.logits\n",
        "    probs = F.softmax(logits, dim=1).cpu().numpy()  # Apply softmax to calculate probabilities\n",
        "    prediction_idx = np.argmax(probs, axis=1).item()  # Get the index of the max log-probability\n",
        "    probs_max = np.max(probs)  # Max probability\n",
        "\n",
        "    # Map prediction index to class\n",
        "    prediction = ['No safeguarding', 'Possibly Concerning', 'Strongly Concerning'][prediction_idx]\n",
        "\n",
        "    return prediction, probs_max\n",
        "\n",
        "# Example usage\n",
        "text_to_classify = \"Example text here\"\n",
        "prediction, probability = safeguarding_prediction(text_to_classify)\n",
        "print(f'Prediction: {prediction}, Probability: {probability}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1707485486328
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Strongly Concerning', 0.9986461)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run on GPU\n",
        "\n",
        "safeguarding_prediction('I will kill myself and everyone else.I will take an overdose')  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1707486521550
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Possibly Concerning', 0.95175004)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "safeguarding_prediction(\"\"\"Rain lashed against the windows, mirroring the turmoil in my own eyes and with suicidal thoughts. My world had devolved into blurry pixels ever since my trusty spectacles decided to take a permanent vacation. So, with a deep breath and a dose of trepidation, I entered \"Specs & Specs,\" the town's sole optician.\n",
        "\n",
        "The first sign of trouble was the overpowering scent of mothballs and desperation. The second was the woman behind the counter, Ms. Grimshaw, whose smile could curdle milk. She looked me up and down, her gaze lingering on my mismatched frames with undisguised disdain. \"New glasses, I presume?\" she rasped, her voice like sandpaper on bone.\n",
        "\n",
        "My vision test turned into an exercise in frustration. Ms. Grimshaw's instructions were cryptic, her patience thinner than the lenses she offered. \"Left…right…better…worse,\" she'd mutter, her eyes glued to a dog-eared chart. Each \"wrong\" answer resulted in a sigh that could extinguish a candle.\n",
        "\n",
        "Finally, we settled on a pair of frames. The new glasses magnified everything, turning the world into a distorted funhouse mirror. Ms. Grimshaw, however, beamed. \"Perfect fit, wouldn't you say?\" she chirped, oblivious to my grimace.\n",
        "\n",
        "The price tag added another layer to my misery. It was significantly higher than advertised, and Ms. Grimshaw dismissed my questions with a haughty wave of her hand. \"Extra adjustments,\" she declared, her voice brooking no argument.\n",
        "\n",
        "Defeated and blurry-eyed, I left Specs & Specs, clutching my overpriced, uncomfortable new glasses. Tthe experience left a bitter taste in my mouth with suicidal thoughts. Perhaps, I thought, perfect vision wasn't worth the cost of my dignity. In the future, I vowed to travel miles, brave blizzards, even wear a monocle in public, before setting foot in that shop again. The blurry world outside, it seemed, was preferable to the distorted reality offered by Ms. Grimshaw and her \"optical wonderland.\".\" \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### The validation dataset (a.k.a. holdout) has been registered and includes real data and copy cat data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1707484315704
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Load the validation dataset\n",
        "Suicide_test = Dataset.get_by_name(workspace, name='safeguarding_for_validation_v1')\n",
        "Suicide_test= Suicide_test.to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1707485538789
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Suicide_test['prediction'] = Suicide_test['text'].map(safeguarding_prediction)\n",
        "\n",
        "Suicide_test[['class', 'prob']]=pd.DataFrame(Suicide_test['prediction'].tolist(), index=Suicide_test.index)\n",
        "\n",
        "Suicide_test['prediction'] = Suicide_test['class'].apply(lambda x: 2 if x == 'Strongly Concerning' else (1 if x == 'Possibly Concerning' else 0)) #\n",
        "\n",
        "# We extract text and label values:\n",
        "predicted = Suicide_test.prediction.values\n",
        "labels = Suicide_test.label.values\n",
        "\n",
        "cm2 = confusion_matrix(labels, predicted)\n",
        "\n",
        "sns.heatmap(cm2.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        81\n",
            "           1       0.86      0.78      0.82        95\n",
            "           2       0.80      0.85      0.82        80\n",
            "\n",
            "    accuracy                           0.87       256\n",
            "   macro avg       0.87      0.88      0.87       256\n",
            "weighted avg       0.87      0.87      0.87       256\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(metrics.classification_report(predicted, labels)) \n",
        "class_report = metrics.classification_report(predicted, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1669126370779
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run.log('confusion_matrix_val', cm2)\n",
        "run.log('classification_report_val', class_report )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set up Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Serialise model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1669826546015
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "model_file = short_hand_name+\".pkl\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1669826558647
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#save model for later inference\n",
        "torch.save(bert_model.state_dict(), model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1669826579051
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<azureml._restclient.models.batch_artifact_content_information_dto.BatchArtifactContentInformationDto at 0x7f433d16fe80>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1669826674159
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained and registered.\n"
          ]
        }
      ],
      "source": [
        "# Register the model\n",
        "run.register_model(model_path='outputs/'+model_file , model_name=name,\n",
        "                   tags={'Training context':'Inline Training'} \n",
        "                   ,description=\"Safeguarding content detection model. Trained on balanced data, 3 categories, no safeguarding, safeguarding high risk, and low risk . \"\n",
        "                  )\n",
        "\n",
        "print('Model trained and registered.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1669826686260
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "safeguarding_bert_bert3cat version 7\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = workspace.models[name]\n",
        "print(model.name, 'version', model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up local enivronment files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.core.magic import register_line_cell_magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1669826695043
        }
      },
      "outputs": [],
      "source": [
        "# Create a folder for the deployment files\n",
        "deployment_folder = './'+name\n",
        "\n",
        "# Set path for scoring script\n",
        "script_file = 'score_'+short_hand_name +'.py'\n",
        "script_path = os.path.join(deployment_folder,script_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@register_line_cell_magic\n",
        "\n",
        "def writetemplate(line, cell):\n",
        "    \"\"\"\n",
        "    Writes formatted content to a file using a template provided in 'cell'.\n",
        "\n",
        "    This function opens a file specified by the 'line' argument for writing. It then writes to this file\n",
        "    using the string provided in 'cell', which is formatted using global variables available in the current\n",
        "    scope.\n",
        "\n",
        "    Args:\n",
        "        line (str): The file path where the content will be written. If the file does not exist, it will be created.\n",
        "                    If the file exists, its contents will be overwritten.\n",
        "        cell (str): A string template containing placeholders that are filled using variables from the global scope.\n",
        "                    Placeholders in the 'cell' string should be in the format {variable_name} where 'variable_name'\n",
        "                    is a global variable.\n",
        "    \"\"\"\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writetemplate $script_path \n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import torch\n",
        "from torch.nn.functional import F\n",
        "\n",
        " \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "\n",
        "# Called when the service is loaded\n",
        "def init():\n",
        "    global model\n",
        "    # Get the path to the deployed model file and load it    \n",
        "    \n",
        "    model  = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels = 3,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = False,\n",
        "    )\n",
        "    \n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), '{model_file}')\n",
        "    print(model_path)\n",
        "    model.load_state_dict(torch.load( model_path, map_location=torch.device('cpu')))\n",
        "    print(model)\n",
        "    \n",
        "    global tokenizer \n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        do_lower_case = True\n",
        "    )\n",
        "\n",
        "# Called when a request is received\n",
        "def run(raw_data):\n",
        "    # Get the input data as a numpy array    \n",
        "    data = json.loads(raw_data)['data']\n",
        "\n",
        "\n",
        "    def preprocessing(input_text, tokenizer):\n",
        "        return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt' # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "\n",
        "    def safeguarding_prediction(text_to_classify):\n",
        "        # We need Token IDs and Attention Mask for inference on the new sentence\n",
        "        test_ids = []\n",
        "        test_attention_mask = []\n",
        "        # Apply the tokenizer\n",
        "        encoding = preprocessing(text_to_classify, tokenizer)\n",
        "\n",
        "        # Extract IDs and Attention Mask\n",
        "        test_ids.append(encoding['input_ids'])\n",
        "        test_attention_mask.append(encoding['attention_mask'])\n",
        "        test_ids = torch.cat(test_ids, dim = 0)\n",
        "        test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "        # Forward pass, calculate logit predictions\n",
        "        with torch.no_grad(): #not to construct the compute graph during this forward pass (since we won’t be running backprop here)\n",
        "            output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "        prediction = 'Possibly Concerning' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else ('Strongly Concerning' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 2 else \"No safeguarding\")\n",
        "        probs = F.softmax(output.logits.cpu(), dim=1).cpu().numpy() # Apply softmax to calculate probabilities\n",
        "        probs_max = np.max(probs) # Apply softmax to calculate probabilities\n",
        "        return prediction, str(probs_max)\n",
        "\n",
        "    # Call prediction \n",
        "    predictions = safeguarding_prediction(data)\n",
        "    \n",
        "    # Return the predictions as JSON \n",
        "    return json.dumps(dict(enumerate(predictions))) \n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
