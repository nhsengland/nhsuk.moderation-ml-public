{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <strong>Warning:</strong> \n",
        "This is here primarily for archiving and transparency purposes. Most of this code will not run as-is. This code was written in January 2023, and used to deploy a complaints detection model. That model is still in use at time of adding this warning. \n",
        "\n",
        "The deployed endpoint in use can be found (LINK REDACTED)\n",
        "\n",
        "This endpoint uses two models in sequence. \n",
        "\n",
        "The model `.pkl` file for the generic, out-of-the-box embeddings generating model (all-mpnet-base-v2) is registered on AMLS (LINK REDACTED)\n",
        "\n",
        "The `.pkl` file for the SVM trained to classify these embeddings is registered (LINK REDACTED)\n",
        "\n",
        "This notebook calls on some functions from a `utils` module. I've pasted a copy of that module in the same folder as this notebook for reference. Note that `utils` is completely defunct, and kept here for archiving purposes alone. \n",
        "\n",
        "Updated context and explanations from February, 2024, will appear in highlighted boxes such as this one. All other markdown is original from the time the notebook was written. \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Complaints Model - Comments only\n",
        "This notebook exists to build and deploy a small ML module which can classify NHS UK reviews into 'complaint' (1) vs 'not a complaint' (0). \n",
        "\n",
        "The notebook is structured as:\n",
        "- Set up\n",
        "- Model build and score\n",
        "- Deployment (ACI and AKS)\n",
        "\n",
        "The basic principle of the model build is a simple chain. \n",
        "\n",
        "1. First of all, a BERT based, pre-trained `pytorch` model encodes the comment text into an embedding.\n",
        "2. Next, that embedding vector is fed to an SVM classifier which yields a result. \n",
        "\n",
        "The model does not use the comment title, nor any of the other features supplied with the data. \n",
        "\n",
        "The dataset used to train and validate the model are hand curated by humans from the NHS UK Reviews team. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set up \n",
        "### Model Choice\n",
        "In the cells below, we importort the model which will be used to get the embeddings for the text to be analysed. At time of writing there are three options here. One of these is a lighter weight, less accurate model which can be used for boosting performance.\n",
        "Of the faster options, one has `cpu` as chosen device. By default, these models will use a GPU if available. However, the cloud resources we deploy to do not have GPU's. So, use the default version for development, and the `device=cpu` version for deployment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1672842835745
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from azureml.core import Workspace, Dataset\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import sentence_transformers\n",
        "import utils\n",
        "\n",
        "subscription_id = 'REDACTED'\n",
        "resource_group = 'REDACTED'\n",
        "workspace_name = 'REDACTED'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "model_sentence_transformer  = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2') #FASTER\n",
        "model_sentence_transformer  = sentence_transformers.SentenceTransformer('all-mpnet-base-v2') #BETTER\n",
        "model_sentence_transformer_cpu  = sentence_transformers.SentenceTransformer('all-mpnet-base-v2', device='cpu') #BETTER\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1672842862897
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "experiment_name = 'complaints_svm_4_experiment'\n",
        "\n",
        "short_hand_name = \"complaints_svm_4\"\n",
        "\n",
        "experiment = Experiment(workspace = workspace, name = experiment_name)\n",
        "\n",
        "# # Start logging data from the experiment\n",
        "run = experiment.start_logging(snapshot_directory=None)\n",
        "run.display_name = experiment_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Make and save data\n",
        "The cell below generates the training and validation splits. Commented out to avoid variation between runs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load Data\n",
        "Varius data sources have been supplied at  different stages. Here we gather them together into one total dataframe, and check for duplication.  If updating the training or validation data, just import to the same names and run the script as usual. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1672842917470
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/standard-gpu/code/Users/daniel.goldwater1/nhsuk.automoderation.datascience/complaints_dg/utils.py:114: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Is_Complaint'] = 1\n",
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/standard-gpu/code/Users/daniel.goldwater1/nhsuk.automoderation.datascience/complaints_dg/utils.py:115: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n",
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/standard-gpu/code/Users/daniel.goldwater1/nhsuk.automoderation.datascience/complaints_dg/utils.py:122: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns={'Feature': 'Comment_Text', 'Complaints': 'Is_Complaint'}, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "df_complaints_dec  = pandas.read_csv('complaints_dec.csv')\n",
        "df_complaints_dec = df_complaints_dec.rename(columns={\n",
        "    'Comment Text':'Comment_Text',\n",
        "    })\n",
        "df_complaints_dec = df_complaints_dec.dropna(subset=['Comment_Text'])\n",
        "df_complaints_dec['Is_Complaint'] = 1\n",
        "\n",
        "dataset = Dataset.get_by_name(workspace=workspace, name='published_eighty_k_1')\n",
        "df_published_80_k = dataset.to_pandas_dataframe()\n",
        "df_published_80_k.rename(columns={\n",
        "    'Comment Text':'Comment_Text',\n",
        "    }, inplace=True)\n",
        "df_published_80_k['Is_Complaint'] = 0\n",
        "del dataset\n",
        "\n",
        "complaints_v1 = utils.get_and_clean_complaints_v1()\n",
        "\n",
        "\n",
        "original_complaints = utils.get_original_complaints_data()\n",
        "\n",
        "df_complaints = pd.concat([\n",
        "    complaints_v1,\n",
        "    df_complaints_dec,\n",
        "    #  original_complaints\n",
        "     ])\n",
        "df_complaints = df_complaints[df_complaints['Is_Complaint']==1]\n",
        "\n",
        "df_complaints['Is_Complaint'].value_counts()\n",
        "# df_complaints.duplicated(subset='Comment_Text').sum()\n",
        "df_total = pd.concat([df_published_80_k, df_complaints] ).drop_duplicates(subset=['Comment_Text'])\n",
        "df_total['Is_Complaint'].value_counts()\n",
        "print(len(df_total))\n",
        "df_total = df_total[['Is_Complaint','Comment_Text']]\n",
        "df_total.dropna(inplace=True)\n",
        "print(len(df_total))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Pre Processing Data\n",
        "Here we clean up the strings. There is a function 'clean_string' which corrects spelling. Further testing shows that spelling correction doesn't really help, so we use the `no_spell` version. We parralellise the processing because large dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1671541185278
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pandarallel import pandarallel\n",
        "import multiprocessing\n",
        "import textblob\n",
        "import re\n",
        "# pandarallel.initialize(progress_bar=True)\n",
        "\n",
        "\n",
        "def clean_string(s):\n",
        "    s = s.strip()\n",
        "    s = re.sub('  ', ' ', s)\n",
        "    s = re.sub('   ', ' ',  s)\n",
        "    s = re.sub('\\n', '', s)\n",
        "    s = s.lower()\n",
        "    return str(textblob.TextBlob(s).correct())\n",
        "\n",
        "def clean_string_nospell(s):\n",
        "    s = s.strip()\n",
        "    s = re.sub('  ', ' ', s)\n",
        "    s = re.sub('   ', ' ',  s)\n",
        "    s = re.sub('\\n', '', s)\n",
        "    s = s.lower()\n",
        "       \n",
        "    return s\n",
        "\n",
        "\n",
        "POOLSIZE = multiprocessing.cpu_count()\n",
        "\n",
        "def parallelize_dataframe(df, func):\n",
        "    \n",
        "    df_split = np.array_split(df, POOLSIZE)\n",
        "    with multiprocessing.Pool(POOLSIZE) as p:\n",
        "        df = pd.concat(p.map(func, df_split))\n",
        "    return df\n",
        "\n",
        "def clean_df_strings(df):\n",
        "    df['cleaned_Comment'] = df['Comment_Text'].apply(clean_string)\n",
        "    return df\n",
        "\n",
        "def clean_df_strings_no_spell(df):\n",
        "    df['cleaned_comment_no_spell'] = df['Comment_Text'].apply(clean_string_nospell)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# df_total = parallelize_dataframe(df_total, clean_df_strings_no_spell)\n",
        "# df_total.to_pickle( 'dataframes/df_total.pkl')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Encode the cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1672842924517
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# df_total['encodings_from_cleaned_no_spell'] = df_total['cleaned_comment_no_spell'].apply(model_sentence_transformer.encode)\n",
        "# df_total.to_pickle( 'dataframes/df_train_cleanup.pkl')\n",
        "df_total = pd.read_pickle( 'dataframes/df_train_cleanup.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Split Data\n",
        "\n",
        "We break the data here into different datasets. Fixing the random state makes this reproducible. \n",
        "\n",
        "We end up with the following dataframes:\n",
        "- `df_validation`: a balanced set of `VALIDATION_SIZE`\n",
        "- `df_train`: a balanced set, whose size is determined by the number of complaints left over after the validation complaints are taken out. \n",
        "- `df_big_val` : A superset of `df_validation`, where we've included all the published reviews except for those in the training or supplementary sets. \n",
        "- `df_published_supplementary` : 20,00 published reviews which are held back from the other datasets. These will be used to supplement the training data. \n",
        "\n",
        "Things to be aware of:\n",
        "- `df_big_val` and `df_validation` have overlap, one being the superset of the other. **THIS IS IMPORTANT**. Don't let this give you confused performance results. \n",
        "- All of the reviews marked as not being complaints have here been filtered to not contain the words 'complain' etc. This is a very crude way of beginning to address label noise in the data. \n",
        "- All of the rows in all of these datasets are 'real'; none of these contain augmented data. All have been cleaned, and all have columns for the encodings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1672842932369
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def drop_complaint_string(df):\n",
        "    return df[~df['Comment_Text'].str.contains('complaint|complain|complained')]\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "df_total = pd.read_pickle('dataframes/df_total.pkl')\n",
        "\n",
        "def make_datasets_from_total(df_total):\n",
        "    VALIDATION_SIZE = 300\n",
        "    df_published = df_total[df_total['Is_Complaint'] == 0]\n",
        "    df_complaints = df_total[df_total['Is_Complaint'] == 1]\n",
        "    df_published = drop_complaint_string(df_published)\n",
        "\n",
        "    complaints_for_val = df_complaints.sample(n=VALIDATION_SIZE//2, random_state=RANDOM_STATE)\n",
        "    df_complaints = df_complaints.drop(complaints_for_val.index)\n",
        "    \n",
        "    df_published_supplementary = df_published.sample(n=20_000, random_state=RANDOM_STATE)\n",
        "    df_published = df_published.drop(df_published_supplementary.index)\n",
        "\n",
        "    published_for_train = df_published.sample(n=len(df_complaints), random_state=RANDOM_STATE)\n",
        "    df_published = df_published.drop(published_for_train.index)\n",
        "\n",
        "    df_train = pd.concat([df_complaints, published_for_train]).sample(frac=1, random_state=RANDOM_STATE)\n",
        "\n",
        "    pub_for_val = df_published.sample(n=VALIDATION_SIZE//2, random_state=RANDOM_STATE)\n",
        "    df_validation = pd.concat([complaints_for_val, pub_for_val]).sample(frac=1, random_state=RANDOM_STATE)\n",
        "    df_big_val = pd.concat([complaints_for_val, df_published])\n",
        "    \n",
        "    df_published = df_published.drop(pub_for_val.index)\n",
        "\n",
        "    return df_train, df_validation, df_published_supplementary, df_big_val\n",
        "\n",
        "df_train, df_validation, df_published_supplementary, df_big_val = make_datasets_from_total(df_total)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Incorporating augmented data\n",
        "Below we load in three datasets which were created with nlp data augmentation. Each of these was produced using slightly different techniques, by Alice Tapper. We will use these to supplement our training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1672751424799
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# df_gen_shuffle =  Dataset.get_by_name(workspace, name='complaints_generated_shuffle')\n",
        "# df_gen_shuffle =  df_gen_shuffle.to_pandas_dataframe()\n",
        "\n",
        "# df_gen_embed =  Dataset.get_by_name(workspace, name='complaints_generated_embeddings')\n",
        "# df_gen_embed =  df_gen_embed.to_pandas_dataframe()\n",
        "\n",
        "# df_gen_para =  Dataset.get_by_name(workspace, name='complaints_generated_paraphrased')\n",
        "# df_gen_para =  df_gen_para.to_pandas_dataframe()\n",
        "\n",
        "# def update_generated(df):\n",
        "#     df.rename(columns={'0': 'Comment_Text'}, inplace=True)\n",
        "#     df['Is_Complaint'] = 1\n",
        "#     df = clean_df_strings_no_spell(df)\n",
        "#     df['encodings_from_cleaned_no_spell'] = df['cleaned_comment_no_spell'].apply(model_sentence_transformer.encode)\n",
        "#     return df\n",
        "\n",
        "# for df in [df_gen_embed, df_gen_para, df_gen_shuffle]:\n",
        "#     df = update_generated(df)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1671529743584
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# with open('dataframes/df_gen_embed.pkl', 'wb') as file:\n",
        "#     pickle.dump(df_gen_embed, file)\n",
        "\n",
        "# with open('dataframes/df_gen_para.pkl', 'wb') as file:\n",
        "#     pickle.dump(df_gen_para, file)\n",
        "\n",
        "# with open('dataframes/df_gen_shuffle.pkl', 'wb') as file:\n",
        "#     pickle.dump(df_gen_shuffle, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1672843200024
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('dataframes/df_gen_embed.pkl', 'rb') as file:\n",
        "    df_gen_embed = pickle.load(file)\n",
        "\n",
        "with open('dataframes/df_gen_para.pkl', 'rb') as file:\n",
        "    df_gen_para = pickle.load(file)\n",
        "\n",
        "with open('dataframes/df_gen_shuffle.pkl', 'rb') as file:\n",
        "    df_gen_shuffle =pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Hyper Optimisation\n",
        "Tradtitionally when we talk about hyper parameter optimisation, we are talking about optimising the parameters of the classifier / model alone. Here, however, we have a few other parameters to consider. We have four datasets which we can supplement our training data with: the published supplementary, and the three sets of augmented data. How many of each of these should we be using? \n",
        "\n",
        "Here we turn each of these into a hyperparameter and optimise over them all, as well as the parameters for the classifier itself. \n",
        "\n",
        "## Fitness Function\n",
        "We have extremely imbalanced data (if we look at `df_big_val`). We also have imbalanced real data; there will be many more reviews published than markedd as complaints. We *also* have an imbalanced goal; raising (false) complaints costs the business money, and in a sense a false positive is worse than a false negative here. \n",
        "We address all of these features by using an $f_{\\beta}$ score, which allows us to account for recall and precision, and to weight them differently. \n",
        "\n",
        "## optimising\n",
        "The optimisation takes over five hours to run. For each set of parameters we:\n",
        "- Create an augmented training set\n",
        "- Fit a model to that set\n",
        "- Run `df_big_val` through the fitted model\n",
        "- Take the $f_\\beta$ score the model attains on the `df_big_val`\n",
        "- Use this score as the fitness function for the hyperparameter optimisation.\n",
        "\n",
        "To be clear: the model **never** sees the validation data. The validation data is, however, used as a benchmark to help decide how much of the different supplementary sets to use as training data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <strong>Update:</strong> Since writing this notebook and generating a model with it, we have come to understand that this is bad practice. We are effectively fitting the *hyperparameters*. \n",
        "\n",
        "Nonetheless, we ran out of time when creating models with an updated methodology. This model still performed better than any newer model(s) against completely new data, data recorded after this model was created. You can see results of that analysis on the [Confluence page here](https://nhsd-confluence.digital.nhs.uk/display/DAT/DS_233%3A+Model_card_Complaints+LV).\n",
        "\n",
        "In the context of what remains of ths notebook, just take any validation scores with a pinch of salt.     \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1672843202717
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import fbeta_score\n",
        "from hyperopt import hp, fmin, Trials, tpe, STATUS_OK\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, precision_score, f1_score, recall_score, confusion_matrix\n",
        "\n",
        "def evaluate_svm(svm_to_eval, df_val, x_key, y_key):\n",
        "    preds = svm_to_eval.predict(np.array(list(df_val[x_key])))\n",
        "    y_true=list(df_val[y_key])\n",
        "    conf_mat =confusion_matrix(y_true=y_true, y_pred=preds)\n",
        "    # conf_disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
        "    print(conf_mat)\n",
        "    fp = conf_mat[0,1] / (conf_mat[0,0] + conf_mat[0,1])\n",
        "    fn = conf_mat[1,0] / (conf_mat[1,0]  + conf_mat[1, 1]) \n",
        "    # print(f'For an N of {n}')\n",
        "    print(f'We got a false pos rate of {fp:.3f}, and a false neg rate of {fn:.3f}')\n",
        "    # conf_disp.plot()\n",
        "    # print(f'for N={n}, we got precision={precision_score(y_true=y_true, y_pred=preds):.2f}, recall={recall_score(y_true=y_true, y_pred=preds)}')\n",
        "    print(classification_report(y_true=y_true, y_pred=preds))\n",
        "\n",
        "\n",
        "def make_augmented_training_df(space):\n",
        "    df = pd.concat([\n",
        "            df_train,\n",
        "            df_published_supplementary.sample(n=space['N_pub'], random_state=42),\n",
        "            df_gen_shuffle.sample(n=space['N_shuffle'], random_state=42),\n",
        "            df_gen_para.sample(n=space['N_para'], random_state=42),\n",
        "            df_gen_embed.sample(n=space['N_embed'], random_state=42),\n",
        "        ])\n",
        "    return df\n",
        "\n",
        "\n",
        "def eval_total_argmin(argmin):\n",
        "    df_augmented=make_augmented_training_df(argmin)\n",
        "\n",
        "    svm_subspace ={k: argmin[k] for k in ('C', 'gamma')}\n",
        "    svm= SVC(**svm_subspace)\n",
        "    svm.fit(\n",
        "            X=np.array(list(df_augmented['encodings_from_cleaned_no_spell'])),\n",
        "            y=list(df_augmented['Is_Complaint'])\n",
        "        ) \n",
        "    evaluate_svm(\n",
        "        svm,\n",
        "        df_big_val,\n",
        "        'encodings_from_cleaned_no_spell',\n",
        "        'Is_Complaint'\n",
        "    )\n",
        "\n",
        "\n",
        "def optimise_svm_with_augmented_and_svm_space():\n",
        "    space_augments = {\n",
        "        'N_pub': hp.choice('N_pub', np.arange(1, len(df_published_supplementary))),\n",
        "        'N_para': hp.choice('N_para', np.arange(1, len(df_gen_para))),\n",
        "        'N_shuffle': hp.choice('N_shuffle', np.arange(1, len(df_gen_shuffle))),\n",
        "        'N_embed': hp.choice('N_embed', np.arange(1, len(df_gen_embed))),\n",
        "        'C': hp.lognormal('C', 0, 1.0),\n",
        "        'gamma': hp.lognormal('gamma', 0.00001, 0.1),\n",
        "    }\n",
        "    \n",
        "    def objectives(space):\n",
        "        df_train_augmented = make_augmented_training_df(space)\n",
        "        svm_subspace ={k: space[k] for k in ('C', 'gamma')}\n",
        "        svm= SVC(**svm_subspace) \n",
        "        # svm= SVC(space) \n",
        "\n",
        "        # score = cross_val_score(clf_svc, X_train, y_train, cv = 5, scoring='f1').mean()\n",
        "        svm.fit(\n",
        "            X=np.array(list(df_train_augmented['encodings_from_cleaned_no_spell'])),\n",
        "            y=list(df_train_augmented['Is_Complaint'])\n",
        "        )\n",
        "\n",
        "        preds= svm.predict(np.array(list(df_big_val['encodings_from_cleaned_no_spell'])))\n",
        "        \n",
        "        scoresz = fbeta_score(y_true=df_big_val['Is_Complaint'], y_pred=preds, beta=2) #CHANGE BETA HERE TO CHANGE THE TARGET METRIC\n",
        "       \n",
        "        return {'loss': -scoresz, 'status': STATUS_OK}\n",
        "    \n",
        "    trials = Trials()\n",
        "\n",
        "    argmin = fmin(objectives, space=space_augments, algo=tpe.suggest, max_evals=150, trials=trials)\n",
        "    \n",
        "    return argmin\n",
        "\n",
        "\n",
        "# argmin_total_fbeta_2 = optimise_svm_with_augmented_and_svm_space()\n",
        "# eval_total_argmin(argmin_total_fbeta_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1672843207589
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "argmin_total_fbeta_2 = {\n",
        "    'C': 16.43268090770241,\n",
        "    'N_embed': 586,\n",
        "    'N_para': 634,\n",
        "    'N_pub': 18381,\n",
        "    'N_shuffle': 226,\n",
        "    'gamma': 3.2719040023652655\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1672843482719
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def make_and_fit_augmented_model(space):\n",
        "    df_train_augmented = make_augmented_training_df(space)\n",
        "    svm_subspace ={k: space[k] for k in ('C', 'gamma')}\n",
        "    svm= SVC(**svm_subspace) \n",
        "    svm.fit(\n",
        "        X=np.array(list(df_train_augmented['encodings_from_cleaned_no_spell'])),\n",
        "        y=list(df_train_augmented['Is_Complaint'])\n",
        "    )\n",
        "    return svm\n",
        "\n",
        "svm_fbeta_2 = make_and_fit_augmented_model(argmin_total_fbeta_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1672844003997
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[58113   463]\n",
            " [   27   123]]\n",
            "We got a false pos rate of 0.008, and a false neg rate of 0.180\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     58576\n",
            "           1       0.21      0.82      0.33       150\n",
            "\n",
            "    accuracy                           0.99     58726\n",
            "   macro avg       0.60      0.91      0.67     58726\n",
            "weighted avg       1.00      0.99      0.99     58726\n",
            "\n"
          ]
        }
      ],
      "source": [
        "eval_total_argmin(argmin_total_fbeta_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1672844291897
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def find_false_positives(svm_to_eval, df_val, x_key='encodings_from_cleaned_no_spell', y_key='Is_Complaint' ):\n",
        "    df_out = pd.DataFrame(columns=['Mistake','Prob','Text'])\n",
        "    for index, row in df_val.iterrows():\n",
        "        pred = svm_to_eval.predict(np.array([row[x_key]]))\n",
        "        if (pred == 1) & (row[y_key] == 0):\n",
        "            df_out = pd.concat([\n",
        "                df_out,\n",
        "                pd.DataFrame({\n",
        "                'Mistake': 'False Pos',\n",
        "                'Prob': svm_to_eval.decision_function(np.array([row[x_key]])),\n",
        "                'Text' : row['Comment_Text']\n",
        "            })])\n",
        "        if (pred==0) & (row[y_key]==1):\n",
        "            df_out = pd.concat([\n",
        "                df_out,\n",
        "                pd.DataFrame({\n",
        "                'Mistake': 'False Neg',\n",
        "                'Prob': svm_to_eval.decision_function(np.array([row[x_key]])),\n",
        "                'Text' : row['Comment_Text']\n",
        "            })])\n",
        "    return df_out\n",
        "\n",
        "df_mistakes_fbeta_2 = find_false_positives(svm_fbeta_2, df_big_val)\n",
        "df_mistakes_fbeta_2.to_csv('Mistakes_from_complaints_4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <strong>Warning:</strong>\n",
        "\n",
        "As stated in the warning cell at the top of this notebook, much of the the practice in this notebook has been superseded by more recent work. The model file which this notebook produced however, is still in use, so it's been worth preserving this. \n",
        "\n",
        "From this point onwards, the notebook deals exclusively with registering and deploying the model. This deployment is **not** still in use. The current version is linked in the warning box at the top of this notebook. \n",
        "\n",
        "I would advise against paying much attention to the notebook beyond this point. More relevant code can be found in the deployment scripts. \n",
        "\n",
        "\n",
        "\n",
        "(REDACTED FROM THIS POINT ONWARDS)\n",
        "\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
