{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings benchmark"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was created to evaluate the performance of various transformer models in generating embeddings for a dataset of comments, reviews on NHS.UK. \n",
        "- The key objective is to measure and analyze the time taken to generate embeddings for different sizes of text samples. This analysis helps in understanding the computational efficiency and performance trade-offs between different models.\n",
        "\n",
        "It loads a reviews dataset from an Azure ML workspace (published reviews), prepares a  random sample  of the data, and uses a range of pre-trained transformer models to perform text embeddings. These models were the best performing on the Hugging Face leaderboard and were explored as we tested their performance when we were in the quest of finding the best embedding models to be used on our classification tasks.\n",
        "The process is ran twice, once using a CPU basic compute on a smaller sample which simulates the deployment capabilities and also on GPU to do it on a bigger dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1694622039266
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "subscription_id = #REDACTED\n",
        "resource_group = #REDACTED\n",
        "workspace_name = #REDACTED\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "dataset = Dataset.get_by_name(workspace, name='published_10k_subset')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1694622046132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset.to_pandas_dataframe()\n",
        "sample = df.loc[0:5, ['Comment Text']]\n",
        "sample100 = df.loc[0:100, ['Comment Text']]\n",
        "sample2k = df.loc[0:2000, ['Comment Text']]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1694622071537
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_models = [\"BAAI/bge-large-en\",\n",
        "                     \"BAAI/bge-base-en\",\n",
        "                     \"BAAI/bge-small-en\", \n",
        "                     \"thenlper/gte-large\", \n",
        "                     \"thenlper/gte-base\", \n",
        "                     \"sentence-transformers/all-mpnet-base-v2\", \n",
        "                     \"intfloat/e5-large-v2\"]"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1694622682811
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store the tokenizer and model objects for each embedding\n",
        "embeddings_dict = {}\n",
        "\n",
        "for embedding in embeddings_models:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(embedding)\n",
        "    model = AutoModel.from_pretrained(embedding)\n",
        "    \n",
        "    embeddings_dict[embedding] = {\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"model\": model\n",
        "    }\n",
        "\n",
        "# Iterate over the embeddings and encode the texts\n",
        "for embedding, embedding_dict in embeddings_dict.items():\n",
        "\n",
        "    # List to store timings for current embedding\n",
        "    timings = []\n",
        "\n",
        "    for text in sample100['Comment Text']:\n",
        "        start_time = time.time()\n",
        "        # Prefix text with \"query: \" if using \"intfloat/e5-large-v2\" embedding\n",
        "        if embedding == \"intfloat/e5-large-v2\":\n",
        "            text = \"query: \" + text\n",
        "\n",
        "        # Specify the maximum length as 512 to be on comparative terms with other embeddings which have a maximum cut-off 512\n",
        "        tokens = embedding_dict[\"tokenizer\"](text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        embeddings_out = embedding_dict[\"model\"](**tokens)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        timings.append(end_time - start_time)\n",
        "    \n",
        "    # Add timings as a new column to the dataframe\n",
        "    sample100[embedding] = timings\n",
        "\n",
        "\n",
        "# Save the DataFrame\n",
        "sample100.to_csv(\"sample100_with_timings.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5be349e0ebf44322ac8cdfb9e4adad2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/314 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fafcea5078424040bc961bcf0c7ef59a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a30111f13b594f5aa6893509eceeaa89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/711k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4efa51b8eaac4335b7f3ce1586409cb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc67e67b3ba04ac9986d9514f41b6428",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b03c7edc69ec418589d09d3c358a9ebb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1694599825875
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the embedding process multiple times per text, record the timings for each run, and then compute the average for each text.\n",
        "# Create a dictionary to store the tokenizer and model objects for each embedding\n",
        "embeddings_dict = {}\n",
        "\n",
        "for embedding in embeddings_models:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(embedding)\n",
        "    model = AutoModel.from_pretrained(embedding)\n",
        "    \n",
        "    embeddings_dict[embedding] = {\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"model\": model\n",
        "    }\n",
        "\n",
        "# Number of times to embed each text to calculate average time\n",
        "num_runs = 10\n",
        "\n",
        "# Iterate over the embeddings and encode the texts\n",
        "for embedding, embedding_dict in embeddings_dict.items():\n",
        "\n",
        "    # List to store average timings for current embedding\n",
        "    avg_timings = []\n",
        "\n",
        "    for text in sample2k['Comment Text']:\n",
        "        \n",
        "        # Prefix text with \"query: \" if using \"intfloat/e5-large-v2\" embedding\n",
        "        if embedding == \"intfloat/e5-large-v2\":\n",
        "            text = \"query: \" + text\n",
        "        \n",
        "        # Store timings for multiple runs\n",
        "        timings_for_text = []\n",
        "\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Specify the maximum length as 512\n",
        "            tokens = embedding_dict[\"tokenizer\"](text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "            embeddings_out = embedding_dict[\"model\"](**tokens)\n",
        "\n",
        "            end_time = time.time()\n",
        "            \n",
        "            timings_for_text.append(end_time - start_time)\n",
        "\n",
        "        # Compute average timing for the text and append to avg_timings\n",
        "        avg_time = sum(timings_for_text) / num_runs\n",
        "        avg_timings.append(avg_time)\n",
        "\n",
        "    # Add average timings as a new column to the dataframe\n",
        "    sample2k[embedding] = avg_timings\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame\n",
        "sample2k.to_csv(\"sample2k_avg.csv\")\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1694613030059
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the embedding process multiple times per text, record the timings for each run, and then compute the average for each text.\n",
        "# Create a dictionary to store the tokenizer and model objects for each embedding\n",
        "embeddings_dict = {}\n",
        "\n",
        "for embedding in embeddings_models:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(embedding)\n",
        "    model = AutoModel.from_pretrained(embedding)\n",
        "    \n",
        "    embeddings_dict[embedding] = {\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"model\": model\n",
        "    }\n",
        "\n",
        "# Number of times to embed each text to calculate average time\n",
        "num_runs = 10\n",
        "\n",
        "# Iterate over the embeddings and encode the texts\n",
        "for embedding, embedding_dict in embeddings_dict.items():\n",
        "\n",
        "    # List to store average timings for current embedding\n",
        "    avg_timings = []\n",
        "\n",
        "    for text in sample100['Comment Text']:\n",
        "        \n",
        "        # Prefix text with \"query: \" if using \"intfloat/e5-large-v2\" embedding\n",
        "        if embedding == \"intfloat/e5-large-v2\":\n",
        "            text = \"query: \" + text\n",
        "        \n",
        "        # Store timings for multiple runs\n",
        "        timings_for_text = []\n",
        "\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Specify the maximum length as 512\n",
        "            tokens = embedding_dict[\"tokenizer\"](text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "            embeddings_out = embedding_dict[\"model\"](**tokens)\n",
        "\n",
        "            end_time = time.time()\n",
        "            \n",
        "            timings_for_text.append(end_time - start_time)\n",
        "\n",
        "        # Compute average timing for the text and append to avg_timings\n",
        "        avg_time = sum(timings_for_text) / num_runs\n",
        "        avg_timings.append(avg_time)\n",
        "\n",
        "    # Add average timings as a new column to the dataframe\n",
        "    sample100[embedding] = avg_timings\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame\n",
        "sample100.to_csv(\"sample100_CPU_avg.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1945682e7877464caaf4fb6b7c714645",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9ac7ff8db764d2a97828afa3f940ae4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a052b3d3c974acbac7081405dcf72d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1181b65b8f641bcb4718b8ab07b1f41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb0790f0ecf54a1993570d84f77d8178",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/363 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00a1c9dcffd64e47a621e49f212790a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49d169d8f96341678574b5bef2e57f4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1e9cf74a2ea4231a1b372d8af520c63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4426f43473c647cb8d7b70871bba7c45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/711k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44b7e4576d8142878b7116be5319f851",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc09fab2d35240848e5a0be71a2579b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/314 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23bb25716cdc4fefacb3c2e665fd3a35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1694623971258
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}