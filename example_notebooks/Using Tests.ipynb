{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tests\n",
    "\n",
    "Pytest is written with the idea that you're building software. The logic of this is that every time that you're submitting a PR or whatever, you want to check that all of those modules are working as expected with each other.\n",
    "\n",
    "Our situation is different - we're more interested in checking that our *assets* are consistent, and that they're working as we expect. I mean our data, models, and environments. \n",
    "\n",
    "So the usual approach of just running the whole test suite isn't right for us. We want to update our test parameters, and run the relevant tests, each time we're about to build or deploy a model. \n",
    "\n",
    "We've built some utility functions to that end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's say that we're about to run some embeddings experiment with two datasets, and we're only interested in looking at the mpnet model.\n",
    "\n",
    "First of all, let's run the appropriate functions which will write those values into our test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils_for_tests import utils_for_tests\n",
    "utils_for_tests.define_list_of_datasets_for_tests(\n",
    "    ['dg_uninformative_scrapset', 'dg_published_scrapset'])\n",
    "\n",
    "\n",
    "utils_for_tests.define_list_of_models_for_tests(list_of_model_names=[\n",
    "    \"all-mpnet-base-v2\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you navigate to `src/utils_for_tests/test_parameters.json` you should see that these values have been written into our test parameters. \n",
    "\n",
    "Now we're ready to run our tests. \n",
    "\n",
    "If you look into the tests folder, you'll notice that we've seperated out the different types / uses of tests into different files. For now let's assume that we're interested in testing the data which we listed above. This will mean that we run `test_data.py` only, instead of the entire `tests/` folder. \n",
    "\n",
    "Let's also assume that we only want to run the rest of the experiment if the tests pass. We can use the exit code for this. \n",
    "\n",
    "Small note - notebooks can be a bit weird, and if you're not seeing proper pytest output from the cell below, try running the equivalent code from a .py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================= test session starts ==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\u001b[0m\n",
      "platform linux -- Python 3.8.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /mnt/batch/tasks/shared/LS_root/mounts/clusters/dg-parallel-10a/code/Users/daniel.goldwater1/ratings-and-reviews-automod\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-3.6.2\n",
      "collected 9 items\n",
      "\n",
      "../tests/test_data.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================== \u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 22.38s\u001b[0m\u001b[32m ==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\u001b[0m\n",
      "All tests passed! We can now run the experiment\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import sys\n",
    "\n",
    "# Set up etc. \n",
    "\n",
    "test_data_file_path = os.path.join(root_path, 'tests/test_data.py')\n",
    "\n",
    "test_exit_code = pytest.main([test_data_file_path])\n",
    "\n",
    "if test_exit_code == 0:\n",
    "    print('All tests passed! We can now run the experiment')\n",
    "else:\n",
    "    print(f'The tests failed with exit code {test_exit_code}. Check output / test logs for more info. Aborting experiment')\n",
    "    \n",
    "    sys.exit(test_exit_code)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
