{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\" ))\n",
        "sys.path.insert(0, root_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This approach is build using the same logic as the other modules - going for composition, and trying to make things easy to refactor. We again have a class which does most of the work. Again, we have a little folder full of `.txt` files which we can use to instruct the LLMs we work with. For the example case, I've written one for complaints detection. As always, this is an example and a good prompt would be one developed with Lee.\n",
        "\n",
        "Let's spin up an instance of our question answering classifier object, and look at that prompt. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1692199293786
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from src.question_answering_approach import question_answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Claude:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anth = question_answering.AnthropicClassifier(pre_prompt_name=\"Complaints_detector_1\")\n",
        "print(anth.pre_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK. So now let's test this out by giving it a review to classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "review_to_classify = \"I had a horrific experience at the Greenfield Medical Centre recently. I went in for a routine check-up and left feeling violated and unsafe. During my appointment, the receptionist stole my purse from my bag. I only realized it when I got home and found my bag open and my money missing. I couldn't believe that a member of the clinic's staff would stoop so low as to steal from patients. It was a clear case of criminality, and I felt utterly disgusted and violated. This incident has shattered my trust in this practice, and I strongly advise everyone to stay away from Greenfield Medical Centre.\"\n",
        "ans = anth.classify_single_review(review=review_to_classify)\n",
        "\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far so good. Importantly, we're getting back a response in the form which we want; a single binary digit. \n",
        "\n",
        "Now let's try this on some more data. I'm going to load in some generated data and run it through the model. Note again the housekeeping we need to do; adding the `train test val` split to the df. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1692199566897
        }
      },
      "outputs": [],
      "source": [
        "from src.data_ingestion import data_ingestion\n",
        "\n",
        "dataset_name = \"complaints_gen_35turbo_context1_v6_3600\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1692199580137
        }
      },
      "outputs": [],
      "source": [
        "data_ingestion.add_train_test_val_labels_to_df(dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1692199588584
        }
      },
      "outputs": [],
      "source": [
        "df = data_ingestion.DataRetrieverDatastore(dataset_name).dataset\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK now we want to do the analysis on this. \n",
        "\n",
        "Again, we've paramaterised this as far as possible. Give the thing lists of the datasets you want analysed. Also, you can say whether you want things balanced or not. You can also specify `n_to_sample` if you don't want to classify the entire dataset - like here! \n",
        "\n",
        "Note that, unlike the embeddings approach, we're not re-registering dataframes here. That's because we're not really doing anything computationally intensive and repeatable, we're just getting a classification given a prompt and a review. \n",
        "\n",
        "For the thing below, every time it gets a new response for a review it'll print a `.`, a kind of progress bar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anth.classify_datasets(\n",
        "    positive_label_dataset_name_list=[dataset_name],\n",
        "    negative_label_dataset_name_list=[],\n",
        "    y_column_name=\"Is Complaint\",\n",
        "    name_of_column_to_classify=\"Comment Text\",\n",
        "    train_test_val_label=\"test\",\n",
        "    n_to_sample=20,\n",
        "    balance_data=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(anth.preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that we gave it entirely generated complaints, we would have been hoping for a list of all 1's here!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anth.get_assessor()\n",
        "anth.assessor.get_and_display_confusion_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not looking too hot. Remember that:\n",
        "- this isn't real data\n",
        "- Have not engineered the prompt *at all* with Lee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# todo Must do logging here!"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
